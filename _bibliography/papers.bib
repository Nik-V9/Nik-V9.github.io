---
---

@article{keetha2023anyloc,
  title = {AnyLoc: Towards Universal Visual Place Recognition},
  author = {Nikhil Keetha* and Avneesh Mishra* and Jay Karhade* and Krishna Murthy Jatavallabhula and Sebastian Scherer and Madhava Krishna and Sourav Garg},
  journal = {IEEE RA-L},
  year = {2023},
  website = {https://anyloc.github.io/},
  pdf = {https://arxiv.org/pdf/2308.00688.pdf},
  code = {https://github.com/AnyLoc/AnyLoc},
  video = {https://youtu.be/ITo8rMInatk},
  preview = {gif/anyloc.gif},
  abstract = {Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR -- a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher performance than existing approaches. We further obtain a 6\% improvement in performance by characterizing the semantic properties of these features, uncovering unique domains which encapsulate datasets from similar environments. Our detailed experiments and analysis lay a foundation for building VPR solutions that may be deployed anywhere, anytime, and across anyview. We encourage the readers to explore our project page and interactive demos: https://anyloc.github.io/.},
  thread = {https://twitter.com/Nik__V__/status/1686814252026286080?s=20},
  selected = {true},
}

@inproceedings{keetha2022airobject,
  title = {AirObject: A Temporally Evolving Graph Embedding for Object Identification},
  author = {Nikhil Keetha and Chen Wang and Yuheng Qiu and Kuan Xu and Sebastian Scherer},
  booktitle = {CVPR},
  year = {2022},
  blog = {https://theairlab.org/airobject/},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/html/Keetha_AirObject_A_Temporally_Evolving_Graph_Embedding_for_Object_Identification_CVPR_2022_paper.html},
  poster = {cvpr22_airobject_poster.pdf},
  code = {https://github.com/Nik-V9/AirObject},
  preview = {gif/airobject_teaser.gif},
  abstract = {Object encoding and identification are vital for robotic tasks such as autonomous exploration, semantic scene understanding, and re-localization. Previous approaches have attempted to either track objects or generate descriptors for object identification. However, such systems are limited to a "fixed" partial object representation from a single viewpoint. In a robot exploration setup, there is a requirement for a temporally "evolving" global object representation built as the robot observes the object from multiple viewpoints. Furthermore, given the vast distribution of unknown novel objects in the real world, the object identification process must be class-agnostic. In this context, we propose a novel temporal 3D object encoding approach, dubbed AirObject, to obtain global keypoint graph-based embeddings of objects. Specifically, the global 3D object embeddings are generated using a temporal convolutional network across structural information of multiple frames obtained from a graph attention-based encoding method. We demonstrate that AirObject achieves the state-of-the-art performance for video object identification and is robust to severe occlusion, perceptual aliasing, viewpoint shift, deformation, and scale transform, outperforming the state-of-the-art single-frame and sequential descriptors. To the best of our knowledge, AirObject is one of the first temporal object encoding methods.},
  selected = {true},
}

@article{keetha2021hierarchical,
  title = {A Hierarchical Dual Model of Environment- and Place-specific Utility for Visual Place Recognition},
  author = {Nikhil Keetha and Michael Milford and Sourav Garg},
  journal = {IEEE RA-L & IROS},
  year = {2021},
  pdf = {https://arxiv.org/pdf/2107.02440.pdf},
  code = {https://github.com/Nik-V9/HEAPUtil},
  video = {https://youtu.be/CbAkVsk0KYE},
  preview = {img/VPR_Util.jpg},
  abstract = {Visual Place Recognition (VPR) approaches have typically attempted to match places by identifying visual cues, image regions or landmarks that have high ``utility'' in identifying a specific place. But this concept of utility is not singular - rather it can take a range of forms. In this paper, we present a novel approach to deduce two key types of utility for VPR: the utility of visual cues `specific' to an environment, and to a particular place. We employ contrastive learning principles to estimate both the environment- and place-specific utility of Vector of Locally Aggregated Descriptors (VLAD) clusters in an unsupervised manner, which is then used to guide local feature matching through keypoint selection. By combining these two utility measures, our approach achieves state-of-the-art performance on three challenging benchmark datasets, while simultaneously reducing the required storage and compute time. We provide further analysis demonstrating that unsupervised cluster selection results in semantically meaningful results, that finer grained categorization often has higher utility for VPR than high level semantic categorization (e.g. building, road), and characterise how these two utility measures vary across different places and environments.},
  selected = {true},
}

@article{keetha2020udet,
  title = {U-Det: A bidirectional feature network based U-Net architecture for lung nodule segmentation},
  author = {Nikhil Keetha and Samson P. A. B. and Annavarapu C. S. R.},
  journal = {MDPI Diagnostics},
  year = {2020},
  pdf = {https://arxiv.org/abs/2003.09293},
  code = {https://github.com/Nik-V9/U-Det},
  preview = {img/udet.jpg},
  abstract = {Early diagnosis and analysis of lung cancer involve a precise and efficient lung nodule segmentation in computed tomography (CT) images. However, the anonymous shapes, visual features, and surroundings of the nodule in the CT image pose a challenging problem to the robust segmentation of the lung nodules. This article proposes U-Det, a resource-efficient model architecture, which is an end to end deep learning approach to solve the task at hand. It incorporates a Bi-FPN (bidirectional feature network) between the encoder and decoder. Furthermore, it uses Mish activation function and class weights of masks to enhance segmentation efficiency. The proposed model is extensively trained and evaluated on the publicly available LUNA-16 dataset consisting of 1186 lung nodules. The U-Det architecture outperforms the existing U-Net model with the Dice similarity coefficient (DSC) of 82.82% and achieves results comparable to human experts.},
}

@article{jatavallabhula2023conceptfusion,
  title = {ConceptFusion: Open-set Multimodal 3D Mapping},
  author = {Krishna Murthy Jatavallabhula and others},
  journal = {RSS},
  year = {2023},
  website = {https://concept-fusion.github.io/},
  pdf = {https://arxiv.org/pdf/2302.07241.pdf},
  preview = {gif/conceptfusion.gif},
  abstract = {Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason bout a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of todayâ€™s foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.},
}
