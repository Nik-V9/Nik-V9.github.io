<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Nikhil Varma Keetha</title> <meta name="author" content="Nikhil Varma Keetha"> <meta name="description" content="Personal website for Nikhil Varma Keetha. "> <meta name="keywords" content="nikhil, varma, keetha, robotics, computer-vision, ai, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.jpeg"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nik-v9.github.io/"> <script src="/assets/js/scramble.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service &amp; Outreach</a> </li> <div class="nav-item CV"> <a class="nav-link" href="/assets/pdf/Nikhil_CV.pdf"> CV </a> </div> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Nikhil</span> Varma Keetha </h1> <p class="desc">PhD in Robotics @ <a href="https://www.ri.cmu.edu/" rel="external nofollow noopener" target="_blank">The Robotics Institute, CMU</a> ‚Ä¢ Wander into the unknown üõ£Ô∏è</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-1400.webp"></source> <img src="/assets/img/profile.jpg?f44aa0305db430c98d2d2824ed450e48" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="profile.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p id="email"><script type="text/javascript">emailScramble=new scrambledString(document.getElementById("email"),"emailScramble","ktaehen.ucm@aewrdn.due",[2,5,7,3,6,4,1,15,18,16,17,8,9,13,14,12,11,10,19,21,22,20]);</script><br></p> <p>RI-SQH, CMU</p> <p>1723 Murray Ave</p> <p>Pittsburgh, PA 15232</p> </div> </div> <div class="clearfix"> <p>Hello! I‚Äôm a PhD student at Carnegie Mellon University (CMU) advised by <a href="https://theairlab.org/team/sebastian/" rel="external nofollow noopener" target="_blank">Sebastian Scherer</a>, as a part of the <a href="https://theairlab.org/" rel="external nofollow noopener" target="_blank">AirLab</a>.</p> <p>I have an innate passion for understanding how human perception &amp; navigation works and how we can build real-world deployable autonomous systems with a positive societal impact. In pursuit of this passion, I aim to build efficient perception systems which leverage the inherent temporality and structure of the real world. My research interests lie at the intersection of temporal computer vision, field robotics, cognitive neuroscience, and self-supervised learning.</p> <p>I received a Bachelor‚Äôs degree in Engineering Physics from <a href="https://www.iitism.ac.in/iitismnew/" rel="external nofollow noopener" target="_blank">IIT (ISM) Dhanbad, India</a>. During my undergrad, I had the pleasure to intern at <a href="https://mila.quebec/en/" rel="external nofollow noopener" target="_blank">Mila</a> and <a href="https://research.qut.edu.au/qcr/" rel="external nofollow noopener" target="_blank">QUT Centre for Robotics</a> and was also a <a href="https://riss.ri.cmu.edu/" rel="external nofollow noopener" target="_blank">2021 CMU Robotics Institute Summer Scholar</a>. I‚Äôm grateful to have many wonderful mentors and collaborators!</p> <p>Apart from research, I‚Äôm a huge movie buff and find recreation in cooking, music, anime, and gaming.</p> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 20vh"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Dec 1, 2023</th> <td> <a href="https://spla-tam.github.io/" rel="external nofollow noopener" target="_blank">SplaTAM</a> is now live! </td> </tr> <tr> <th scope="row">Sep 1, 2023</th> <td> Collaborating with <a href="https://www.cs.cmu.edu/~deva/" rel="external nofollow noopener" target="_blank">Deva Ramanan</a> on dynamic reconstruction &amp; tracking in the wild! </td> </tr> <tr> <th scope="row">Aug 1, 2023</th> <td> <a href="https://anyloc.github.io/" rel="external nofollow noopener" target="_blank">AnyLoc</a> is live! </td> </tr> <tr> <th scope="row">Mar 14, 2023</th> <td> <a href="https://theairlab.org/tartanplanningseries/" rel="external nofollow noopener" target="_blank">Tartan Planning Series</a> is now live! </td> </tr> <tr> <th scope="row">Feb 14, 2023</th> <td> <a href="https://concept-fusion.github.io/" rel="external nofollow noopener" target="_blank">ConceptFusion</a> is public! </td> </tr> <tr> <th scope="row">Feb 2, 2023</th> <td> Received CMU RI PhD offer for Fall 2023! </td> </tr> <tr> <th scope="row">Nov 1, 2022</th> <td> Reviewing for CVPR 2023 </td> </tr> <tr> <th scope="row">Oct 15, 2022</th> <td> Reviewed for IEEE ICRA 2023 </td> </tr> <tr> <th scope="row">Sep 1, 2022</th> <td> <a href="https://riss.ri.cmu.edu/robolaunch-2022/" rel="external nofollow noopener" target="_blank">RoboLaunch</a> reached 13,000 students &amp; viewers worldwide! </td> </tr> <tr> <th scope="row">Jun 1, 2022</th> <td> Organized the <a href="https://riss.ri.cmu.edu/robolaunch/" rel="external nofollow noopener" target="_blank">RoboLaunch Initiative</a> at CMU </td> </tr> <tr> <th scope="row">May 30, 2022</th> <td> Graduated B.Tech with Distinction! </td> </tr> <tr> <th scope="row">Apr 11, 2022</th> <td> Reviewed for ECCV 2022 </td> </tr> <tr> <th scope="row">Mar 23, 2022</th> <td> Reviewed for IEEE IROS 2022 </td> </tr> <tr> <th scope="row">Mar 19, 2022</th> <td> Accepted Fully-funded MS in Robotics offer from CMU! </td> </tr> <tr> <th scope="row">Mar 4, 2022</th> <td> <a href="https://theairlab.org/airobject/" rel="external nofollow noopener" target="_blank">AirObject</a> accepted to CVPR 2022! </td> </tr> <tr> <th scope="row">Oct 10, 2021</th> <td> Reviewed for IEEE RA-L &amp; ICRA 2022 </td> </tr> <tr> <th scope="row">Sep 27, 2021</th> <td> Presented <a href="https://youtu.be/CbAkVsk0KYE" rel="external nofollow noopener" target="_blank">HEAPUtil</a> at IROS 2021 </td> </tr> <tr> <th scope="row">Aug 10, 2021</th> <td> Presented <a href="https://youtu.be/pgviYqtlbQk?t=2243" rel="external nofollow noopener" target="_blank">RISS Summer Experience</a> as a part of the 2021 RISS Community Seminar Series </td> </tr> <tr> <th scope="row">Jun 14, 2021</th> <td> <a href="https://ieeexplore.ieee.org/abstract/document/9484750" rel="external nofollow noopener" target="_blank">HEAPUtil</a> accepted to IEEE Robotics and Automation Letters (RA-L) and IROS 2021! </td> </tr> <tr> <th scope="row">May 1, 2021</th> <td> Organizing the <a href="https://theairlab.org/tartanslamseries/" rel="external nofollow noopener" target="_blank">Tartan SLAM Series</a> at CMU Robotics Institute </td> </tr> <tr> <th scope="row">Apr 1, 2021</th> <td> Interning at the <a href="https://theairlab.org/" rel="external nofollow noopener" target="_blank">AirLab</a> CMU mentored by <a href="https://chenwang.site/" rel="external nofollow noopener" target="_blank">Dr. Chen Wang</a> and <a href="https://theairlab.org/team/sebastian/" rel="external nofollow noopener" target="_blank">Prof. Sebastian Scherer</a> </td> </tr> <tr> <th scope="row">Mar 25, 2021</th> <td> Reviewed for IEEE Robotics and Automation Letters (RA-L) &amp; IROS 2021 </td> </tr> <tr> <th scope="row">Mar 18, 2021</th> <td> Selected as a <a href="https://riss.ri.cmu.edu/" rel="external nofollow noopener" target="_blank">CMU Robotics Institute Summer Scholar</a>! </td> </tr> <tr> <th scope="row">Jan 1, 2021</th> <td> Winter 2021 Intern at <a href="https://montrealrobotics.ca/" rel="external nofollow noopener" target="_blank">REAL</a> Mila mentored by <a href="https://krrish94.github.io/" rel="external nofollow noopener" target="_blank">Krishna Murthy</a> and <a href="https://liampaull.ca/" rel="external nofollow noopener" target="_blank">Prof. Liam Paull</a> </td> </tr> <tr> <th scope="row">Nov 1, 2020</th> <td> Received <a href="https://accv2020.github.io/attending/diversity-grant/" rel="external nofollow noopener" target="_blank">ACCV 2020 Diversity Grant</a> </td> </tr> <tr> <th scope="row">Aug 14, 2020</th> <td> Interning at the <a href="https://research.qut.edu.au/qcr/" rel="external nofollow noopener" target="_blank">QUT Centre for Robotics</a> mentored by <a href="https://staff.qut.edu.au/staff/s.garg" rel="external nofollow noopener" target="_blank">Dr. Sourav Garg</a> and <a href="https://www.qut.edu.au/research/michael-milford" rel="external nofollow noopener" target="_blank">Prof. Michael Milford</a> </td> </tr> </table> </div> </div> <br> <h2><a href="/publications/" style="color: inherit;">Featured Publications</a></h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gif/splatam.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gif/splatam.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gif/splatam.gif-1400.webp"></source> <img src="/assets/img/publication_preview/gif/splatam.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="gif/splatam.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="keetha2023splatam" class="col-sm-8"> <div class="title">SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM</div> <div class="author"> <em>Nikhil Keetha</em>,¬†<a href="https://jaykarhade.github.io/" rel="external nofollow noopener" target="_blank">Jay Karhade</a>,¬†<a href="https://krrish94.github.io/" rel="external nofollow noopener" target="_blank">Krishna Murthy Jatavallabhula</a>,¬†<a href="https://gengshan-y.github.io/" rel="external nofollow noopener" target="_blank">Gengshan Yang</a>,¬†<a href="https://theairlab.org/team/sebastian/" rel="external nofollow noopener" target="_blank">Sebastian Scherer</a>,¬†<a href="https://www.cs.cmu.edu/~deva/" rel="external nofollow noopener" target="_blank">Deva Ramanan</a>,¬†and¬†<a href="https://www.vision.rwth-aachen.de/person/216/" rel="external nofollow noopener" target="_blank">Jonathon Luiten</a> </div> <div class="periodical"> <em>Arxiv</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2312.02126.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/spla-tam/SplaTAM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://x.com/Nik__V__/status/1731840557553496410?s=20" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Thread</a> <a href="https://spla-tam.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments show that SplaTAM achieves up to 2X state-of-the-art performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gif/anyloc.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gif/anyloc.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gif/anyloc.gif-1400.webp"></source> <img src="/assets/img/publication_preview/gif/anyloc.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="gif/anyloc.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="keetha2023anyloc" class="col-sm-8"> <div class="title">AnyLoc: Towards Universal Visual Place Recognition</div> <div class="author"> <em>Nikhil Keetha*</em>,¬†<a href="https://theprojectsguy.github.io/" rel="external nofollow noopener" target="_blank">Avneesh Mishra*</a>,¬†<a href="https://jaykarhade.github.io/" rel="external nofollow noopener" target="_blank">Jay Karhade*</a>,¬†<a href="https://krrish94.github.io/" rel="external nofollow noopener" target="_blank">Krishna Murthy Jatavallabhula</a>,¬†<a href="https://theairlab.org/team/sebastian/" rel="external nofollow noopener" target="_blank">Sebastian Scherer</a>,¬†<a href="https://robotics.iiit.ac.in/" rel="external nofollow noopener" target="_blank">Madhava Krishna</a>,¬†and¬†<a href="https://researchers.adelaide.edu.au/profile/sourav.garg" rel="external nofollow noopener" target="_blank">Sourav Garg</a> </div> <div class="periodical"> <em>IEEE RA-L</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2308.00688.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/AnyLoc/AnyLoc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://twitter.com/Nik__V__/status/1686814252026286080?s=20" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Thread</a> <a href="https://youtu.be/ITo8rMInatk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://anyloc.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR ‚Äì a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher performance than existing approaches. We further obtain a 6% improvement in performance by characterizing the semantic properties of these features, uncovering unique domains which encapsulate datasets from similar environments. Our detailed experiments and analysis lay a foundation for building VPR solutions that may be deployed anywhere, anytime, and across anyview. We encourage the readers to explore our project page and interactive demos: https://anyloc.github.io/.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gif/airobject_teaser.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gif/airobject_teaser.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gif/airobject_teaser.gif-1400.webp"></source> <img src="/assets/img/publication_preview/gif/airobject_teaser.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="gif/airobject_teaser.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="keetha2022airobject" class="col-sm-8"> <div class="title">AirObject: A Temporally Evolving Graph Embedding for Object Identification</div> <div class="author"> <em>Nikhil Keetha</em>,¬†<a href="https://sairlab.org/" rel="external nofollow noopener" target="_blank">Chen Wang</a>,¬†<a href="https://www.linkedin.com/in/yuheng-qiu-6bb9151b0" rel="external nofollow noopener" target="_blank">Yuheng Qiu</a>,¬†<a href="https://scholar.google.com/citations?user=-p7HvCMAAAAJ" rel="external nofollow noopener" target="_blank">Kuan Xu</a>,¬†and¬†<a href="https://theairlab.org/team/sebastian/" rel="external nofollow noopener" target="_blank">Sebastian Scherer</a> </div> <div class="periodical"> <em>In CVPR</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Keetha_AirObject_A_Temporally_Evolving_Graph_Embedding_for_Object_Identification_CVPR_2022_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://theairlab.org/airobject/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/Nik-V9/AirObject" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/cvpr22_airobject_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Object encoding and identification are vital for robotic tasks such as autonomous exploration, semantic scene understanding, and re-localization. Previous approaches have attempted to either track objects or generate descriptors for object identification. However, such systems are limited to a "fixed" partial object representation from a single viewpoint. In a robot exploration setup, there is a requirement for a temporally "evolving" global object representation built as the robot observes the object from multiple viewpoints. Furthermore, given the vast distribution of unknown novel objects in the real world, the object identification process must be class-agnostic. In this context, we propose a novel temporal 3D object encoding approach, dubbed AirObject, to obtain global keypoint graph-based embeddings of objects. Specifically, the global 3D object embeddings are generated using a temporal convolutional network across structural information of multiple frames obtained from a graph attention-based encoding method. We demonstrate that AirObject achieves the state-of-the-art performance for video object identification and is robust to severe occlusion, perceptual aliasing, viewpoint shift, deformation, and scale transform, outperforming the state-of-the-art single-frame and sequential descriptors. To the best of our knowledge, AirObject is one of the first temporal object encoding methods.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/img/VPR_Util-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/img/VPR_Util-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/img/VPR_Util-1400.webp"></source> <img src="/assets/img/publication_preview/img/VPR_Util.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="img/VPR_Util.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="keetha2021hierarchical" class="col-sm-8"> <div class="title">A Hierarchical Dual Model of Environment- and Place-specific Utility for Visual Place Recognition</div> <div class="author"> <em>Nikhil Keetha</em>,¬†<a href="https://www.qut.edu.au/research/michael-milford" rel="external nofollow noopener" target="_blank">Michael Milford</a>,¬†and¬†<a href="https://researchers.adelaide.edu.au/profile/sourav.garg" rel="external nofollow noopener" target="_blank">Sourav Garg</a> </div> <div class="periodical"> <em>IEEE RA-L &amp; IROS</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2107.02440.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Nik-V9/HEAPUtil" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://youtu.be/CbAkVsk0KYE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Visual Place Recognition (VPR) approaches have typically attempted to match places by identifying visual cues, image regions or landmarks that have high ‚Äúutility‚Äù in identifying a specific place. But this concept of utility is not singular - rather it can take a range of forms. In this paper, we present a novel approach to deduce two key types of utility for VPR: the utility of visual cues ‚Äòspecific‚Äô to an environment, and to a particular place. We employ contrastive learning principles to estimate both the environment- and place-specific utility of Vector of Locally Aggregated Descriptors (VLAD) clusters in an unsupervised manner, which is then used to guide local feature matching through keypoint selection. By combining these two utility measures, our approach achieves state-of-the-art performance on three challenging benchmark datasets, while simultaneously reducing the required storage and compute time. We provide further analysis demonstrating that unsupervised cluster selection results in semantically meaningful results, that finer grained categorization often has higher utility for VPR than high level semantic categorization (e.g. building, road), and characterise how these two utility measures vary across different places and environments.</p> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6E%6B%65%65%74%68%61@%61%6E%64%72%65%77.%63%6D%75.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=ZTm5H50AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Nik-V9" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/nik-v9" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/Nik__V__" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> The best way to reach me is via email. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 Nikhil Varma Keetha. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 05, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-G7H9CMFQ8S"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-G7H9CMFQ8S");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>